## Libraries Used

- **NumPy**: For numerical operations.
- **Pandas**: For handling and processing tabular data (CSV files).
- **Matplotlib**: For plotting results.
- **PyTorch**: For fine-tuning the GPT-2 model.
- **Transformers**: Hugging Face library for working with pre-trained models like GPT-2.

# Mitigating Text Generation Harms with FairPrism Dataset

## Overview

This project focuses on addressing and mitigating harmful stereotypes and biases in text generation using the **FairPrism dataset**. The goal of this project is to create a prototype chatbot capable of detecting and mitigating fairness-related harms in AI-generated text. By fine-tuning models and utilizing a feedback-driven approach, the chatbot aims to ensure inclusivity, fairness, and ethical AI content generation.

## Contents

1. **count_categories.py**
   - Script for counting the frequency of different demographic categories in the AI-generated text.

2. **fairprism_preprocess.ipynb**
   - Jupyter Notebook for preprocessing the FairPrism dataset, including data cleaning and tokenization.

3. **prompts_preprocess.ipynb**
   - Preprocessing pipeline for user-provided prompts to be evaluated for potential harmful content.

4. **generate.py**
   - Script for generating responses from the fine-tuned GPT-2 model based on the preprocessed input prompts.

5. **training_data.txt**
   - Contains the training data, including user prompts and categorized responses, which are used to fine-tune the GPT-2 model.

6. **fairprism_evaluation.csv**
   - CSV file containing evaluation results of the FairPrism dataset for various demographic categories.

7. **prompts_preprocessed.txt**
   - Contains preprocessed prompts used for training the model.

8. **samples.zip**
   - Contains generated sample responses from the fine-tuned model for evaluation.

9. **usage.py**
   - Script detailing how to use the chatbot for detecting and mitigating harmful content in AI-generated text.

## Key Concepts

- **FairPrism Dataset**: A dataset used to evaluate fairness-related harms in text generation, focusing on gender, sexuality, race, and other demographic considerations.
- **Bias and Harm Detection**: The project focuses on detecting biases and harmful stereotypes in text generated by AI models, particularly GPT-2.
- **Mitigation**: Once biases and harmful content are detected, the chatbot provides feedback and suggestions for improving the fairness of generated content.

## Model Training

The model used in this project is a fine-tuned version of **GPT-2**. It has been trained on the FairPrism dataset to recognize harmful content and provide mitigation advice. Training data includes prompts and categorized outputs that allow the model to identify harmful stereotypes and suggest corrections.

## Usage

1. Preprocess the data using the provided notebooks.
2. Fine-tune the GPT-2 model on the FairPrism dataset using the `generate.py` script.
3. Use the chatbot for real-time feedback on user prompts, focusing on harmful content mitigation.

## Findings

The analysis conducted during the evaluation phase highlights that harmful content is most often categorized under **gender** and **sexuality**, with the **other** category being a significant portion. The model can detect harmful biases, but further improvements are needed to cover a more diverse set of categories.

## Limitations

- **Training Data**: The model was trained on only a portion of the available data due to computational constraints, which may introduce biases in the output.
- **Model**: While GPT-2 provides good results, upgrading to a larger model like GPT-2 1558M could improve detection accuracy and fairness mitigation.
